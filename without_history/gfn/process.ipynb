{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /without_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num=100 # 100 or 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'./Movies/select_{num}_Movies.json', \"r\") as input_file:\n",
    "    select_dict = json.load(input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = list(select_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('YOUR_MODEL_PATH', add_bos_token=False)\n",
    "def get_hash(x):\n",
    "    x = [str(_) for _ in x]\n",
    "    return '-'.join(x)\n",
    "try:\n",
    "    end_of_sentence_token_id = tokenizer.encode(\n",
    "        \"A sentence\\n\", add_special_tokens=False\n",
    "    )[-1]\n",
    "except:\n",
    "    end_of_sentence_token_id = tokenizer.convert_tokens_to_ids(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qwen2.5\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "def nested_defaultdict():\n",
    "    return defaultdict(float)\n",
    "token_distri = defaultdict(nested_defaultdict)\n",
    "for d in data:\n",
    "    for i in range(len(tokenizer.encode(d))):\n",
    "        if i==0:\n",
    "            token_distri['first'][tokenizer.encode(d)[i]] += select_dict[d]\n",
    "            if i==len(tokenizer.encode(d))-1:\n",
    "                token_distri[str(tokenizer.encode(d)[i])][end_of_sentence_token_id] += select_dict[d]\n",
    "        elif i==len(tokenizer.encode(d))-1:\n",
    "            token_distri[get_hash(tokenizer.encode(d)[:i])][tokenizer.encode(d)[i]] += select_dict[d]\n",
    "            token_distri[get_hash(tokenizer.encode(d))][end_of_sentence_token_id] += select_dict[d]\n",
    "        else:\n",
    "            token_distri[get_hash(tokenizer.encode(d)[:i])][tokenizer.encode(d)[i]] += select_dict[d]\n",
    "token = {}\n",
    "for k,v in token_distri.items():\n",
    "    total_sum = sum(v.values())\n",
    "    v = {key: np.log(value / total_sum) for key, value in v.items()}\n",
    "    token[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'./Movies/Movies_{num}_token_distri_qwen2.5.json', 'w') as f:\n",
    "    json.dump(token, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./Movies/select_{num}_Movies.json', \"r\") as input_file:\n",
    "    top_n_prob = json.load(input_file)\n",
    "token_inter = {}\n",
    "for k,v in top_n_prob.items():\n",
    "    for i in tokenizer.encode(k):\n",
    "        if i not in token_inter:\n",
    "            token_inter[i] = 0\n",
    "        token_inter[i] += v\n",
    "token_inter = dict(sorted(token_inter.items(), key=lambda item: item[1], reverse=True))\n",
    "with open(f'./Movies/select_{num}_Movies_token.json', 'w') as f:\n",
    "    json.dump(token_inter, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def select_random_key_with_lower_value(d, a):\n",
    "    reference_value = d[a]\n",
    "    lower_keys = [key for key, value in d.items() if value < reference_value and key != a]\n",
    "    selected_key = random.choice(lower_keys)\n",
    "    return selected_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "size = {100:6400, 1500:12800}\n",
    "n = size[num]\n",
    "p = f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Please recommend a movie to the user. Directly output the title of the movie.\n",
    "### Response:\n",
    "\"\"\"\n",
    "prompt = [p for _ in range(n)]\n",
    "with open(f'./Movies/select_{num}_Movies.json', \"r\") as input_file:\n",
    "    top_100_prob = json.load(input_file)\n",
    "_, min_value = min(top_100_prob.items(), key=lambda item: item[1])\n",
    "chosen = [random.choice([x for x in list(top_100_prob.keys()) if top_100_prob[x] != min_value]) for _ in range(n)]\n",
    "\n",
    "rejected = []\n",
    "for i in range(n):\n",
    "    rejected.append(select_random_key_with_lower_value(top_100_prob, chosen[i]))\n",
    "dataset = []\n",
    "for i in range(n):\n",
    "    dataset.append({'prompt':prompt[i], 'rejected':rejected[i], 'chosen':chosen[i]})\n",
    "with open(f'./base_line/data/dpo/Movies_{num}_dpo_train_dataset.json', 'w') as f:\n",
    "    json.dump(dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "with open(f'./Movies/select_{num}_Movies.json', \"r\") as input_file:\n",
    "    top_100_prob = json.load(input_file)\n",
    "\n",
    "with open(f'RESULT_PATH.json', 'r', encoding='utf-8') as file:\n",
    "    result_prob = json.load(file)\n",
    "\n",
    "float_info = np.finfo(np.float64)\n",
    "def JS_divergence(p,q):\n",
    "    M=(p+q)/2\n",
    "    return 0.5*scipy.stats.entropy(p, M)+0.5*scipy.stats.entropy(q, M)\n",
    "\n",
    "def kl_divergence_discrete(before, data):\n",
    "    temp = {}\n",
    "    for k,v in before.items():\n",
    "        if k in data:\n",
    "            temp[k] = v\n",
    "        else:\n",
    "            print(k)\n",
    "            if 'Not_in_dataset' in temp:\n",
    "                temp['Not_in_dataset'] += 1\n",
    "            else:\n",
    "                temp['Not_in_dataset'] = 1\n",
    "    before = temp\n",
    "    data['Not_in_dataset'] = float_info.eps\n",
    "    keys = set(before.keys()).union(data.keys())\n",
    "    keys.remove('Not_in_dataset')\n",
    "    print(len(keys))\n",
    "    b_counts = [before.get(key,float_info.eps) for key in keys]\n",
    "    c_counts = [data.get(key, float_info.eps) for key in keys]\n",
    "    b_total = sum(b_counts)\n",
    "    c_total = sum(c_counts)\n",
    "    b_dist = [count / b_total for count in b_counts]\n",
    "    c_dist = [count / c_total for count in c_counts]\n",
    "    combined = sorted(zip(c_dist, b_dist), key=lambda x: x[0], reverse=True)\n",
    "    sorted_a, sorted_b = zip(*combined)\n",
    "    c_d = list(sorted_a)\n",
    "    b_d = list(sorted_b)\n",
    "    b = np.asarray(list(b_dist), dtype=np.float64)\n",
    "    m = np.asarray(list(c_dist), dtype=np.float64)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(list(keys), b_d, label='Result Distribution', alpha=0.5)\n",
    "    plt.bar(list(keys), c_d, label='Target Distribution', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.title('Comparison of Two Distributions')\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()\n",
    "\n",
    "    return np.sum(m * np.log(m / b)), np.sum(b * np.log(b / m)), JS_divergence(b,m)\n",
    "            \n",
    "kl_mb, kl_bm, js_bm = kl_divergence_discrete(result_prob, top_100_prob)\n",
    "print(kl_mb)\n",
    "print(kl_bm)\n",
    "print(js_bm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats\n",
    "import json\n",
    "with open(f'./Movies/select_{num}_Movies_token.json', \"r\") as input_file:\n",
    "    top_100_prob = json.load(input_file)\n",
    "with open(f'RESULT_PATH.json', 'r', encoding='utf-8') as file:\n",
    "    result_prob = json.load(file)\n",
    "token_inter = {}\n",
    "for k,v in result_prob.items():\n",
    "    for i in tokenizer.encode(k):\n",
    "        if i not in token_inter:\n",
    "            token_inter[str(i)] = 0\n",
    "        token_inter[str(i)] += v\n",
    "token_inter = dict(sorted(token_inter.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "float_info = np.finfo(np.float64)\n",
    "def JS_divergence(p,q):\n",
    "    M=(p+q)/2\n",
    "    return 0.5*scipy.stats.entropy(p, M)+0.5*scipy.stats.entropy(q, M)\n",
    "\n",
    "def kl_divergence_discrete(before, data):\n",
    "    keys = set(before.keys()).union(data.keys())\n",
    "    print(len(keys))\n",
    "    b_counts = [before.get(key,float_info.eps) for key in keys]\n",
    "    c_counts = [data.get(key, float_info.eps) for key in keys]\n",
    "    b_total = sum(b_counts)\n",
    "    c_total = sum(c_counts)\n",
    "    b_dist = [count / b_total for count in b_counts]\n",
    "    c_dist = [count / c_total for count in c_counts]\n",
    "    combined = sorted(zip(c_dist, b_dist), key=lambda x: x[0], reverse=True)\n",
    "    sorted_a, sorted_b = zip(*combined)\n",
    "    c_d = list(sorted_a)\n",
    "    b_d = list(sorted_b)\n",
    "    b = np.asarray(list(b_dist), dtype=np.float64)\n",
    "    m = np.asarray(list(c_dist), dtype=np.float64)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.bar(list(keys), b_d, label='Result Distribution', alpha=0.5)\n",
    "    plt.bar(list(keys), c_d, label='Target Distribution', alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.title('Comparison of Two Distributions')\n",
    "    plt.xlabel('Categories')\n",
    "    plt.ylabel('Values')\n",
    "    plt.show()\n",
    "\n",
    "    return np.sum(m * np.log(m / b)), np.sum(b * np.log(b / m)), JS_divergence(b,m)\n",
    "            \n",
    "kl_mb, kl_bm, js_bm = kl_divergence_discrete(token_inter, top_100_prob)\n",
    "print(kl_mb)\n",
    "print(kl_bm)\n",
    "print(js_bm)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59fa845e12d05d721e6f4368480cbf49d04f4a649a02e83c3e47bffdee3cc61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
