{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Video_Games'\n",
    "start_year=2015\n",
    "end_year=2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd with_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "def read_list_column_from_csv(train_file_path, column_name):\n",
    "    result = []\n",
    "    with open(train_file_path, mode='r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            list_str = row[column_name]\n",
    "            list_obj = eval(list_str)\n",
    "            result.append(list_obj)\n",
    "    return result\n",
    "\n",
    "train_file_path = f'./train/{category}_5_{start_year}-10-{end_year}-11.csv'\n",
    "column_name = 'history_item_title'\n",
    "list_of_lists = read_list_column_from_csv(train_file_path, column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_to_inter = {}\n",
    "for l in list_of_lists:\n",
    "    for i in l:\n",
    "        if i in item_to_inter:\n",
    "            item_to_inter[i] += 1\n",
    "        else:\n",
    "            item_to_inter[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(f'./info/{category}_title_interaction_5_{start_year}-10-{end_year}-11.json', mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(item_to_inter, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained('YOUR_MODEL_PATH', add_bos_token=False)\n",
    "def get_hash(x):\n",
    "    x = [str(_) for _ in x]\n",
    "    return '-'.join(x)\n",
    "def get_state_chain(x):\n",
    "    x = [str(_) for _ in x]\n",
    "    return '@'.join(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    end_of_sentence_token_id = tokenizer.encode(\n",
    "        \"A sentence\\n\", add_special_tokens=False\n",
    "    )[-1]\n",
    "except:\n",
    "    end_of_sentence_token_id = tokenizer.convert_tokens_to_ids(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n=1\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "def nested_defaultdict():\n",
    "    return defaultdict(float)\n",
    "token_distri = defaultdict(nested_defaultdict)\n",
    "for d,v in item_to_inter.items():\n",
    "    for i in range(len(tokenizer.encode(d))):\n",
    "        print(tokenizer.encode(d))\n",
    "        if i==0:\n",
    "            token_distri['first'][tokenizer.encode(d)[i]] += v\n",
    "            if i==len(tokenizer.encode(d))-1:\n",
    "                token_distri[str(tokenizer.encode(d)[i])][end_of_sentence_token_id] += v\n",
    "        elif i==len(tokenizer.encode(d))-1:\n",
    "            token_distri[get_hash(tokenizer.encode(d)[:i])][tokenizer.encode(d)[i]] += v\n",
    "            token_distri[get_hash(tokenizer.encode(d))][end_of_sentence_token_id] += v\n",
    "        else:\n",
    "            token_distri[get_hash(tokenizer.encode(d)[:i])][tokenizer.encode(d)[i]] += v\n",
    "token = {}\n",
    "for k,v in token_distri.items():\n",
    "    total_sum = sum(v.values())\n",
    "    v = {key: np.log(value / total_sum) for key, value in v.items()}\n",
    "    token[k] = v\n",
    "with open(f'./info/{category}_token_distribution_5_{start_year}-10-{end_year}-11.json', mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(token, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vary Supervision Granularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=1 # 5 10 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "def nested_defaultdict():\n",
    "    return defaultdict(float)\n",
    "token_distri = defaultdict(nested_defaultdict)\n",
    "for d,v in item_to_inter.items():\n",
    "    tokens = tokenizer.encode(d)\n",
    "    state_num = math.ceil(len(tokens)/n)\n",
    "    state_list = [get_hash(tokens[j*n:min(len(tokens),(j+1)*n)]) for j in range(state_num)]\n",
    "    for i in range(state_num):\n",
    "        current_state = state_list[i]\n",
    "        if i==0:\n",
    "            token_distri['first'][current_state] += v\n",
    "            if i==state_num-1:\n",
    "                token_distri[current_state][end_of_sentence_token_id] += v\n",
    "        elif i==state_num-1:\n",
    "            token_distri[get_state_chain(state_list[:i])][current_state] += v\n",
    "            token_distri[get_state_chain(state_list)][end_of_sentence_token_id] += v\n",
    "        else:\n",
    "            token_distri[get_state_chain(state_list[:i])][current_state] += v\n",
    "token = {}\n",
    "for k,v in token_distri.items():\n",
    "    total_sum = sum(v.values())\n",
    "    v = {key: np.log(value / total_sum) for key, value in v.items()}\n",
    "    token[k] = v\n",
    "with open(f'./info/{category}_token_distribution_{n}_5_{start_year}-10-{end_year}-11.json', mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(token, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dict_into_groups(d, group_count=10):\n",
    "    sorted_items = sorted(d.items(), key=lambda x: x[1], reverse=True)\n",
    "    item_count = len(sorted_items)\n",
    "    items_per_group = item_count // group_count\n",
    "    remainder = item_count % group_count\n",
    "\n",
    "    groups = []\n",
    "    start_index = 0\n",
    "    for i in range(group_count):\n",
    "        end_index = start_index + items_per_group + (1 if i < remainder else 0)\n",
    "        group = dict(sorted_items[start_index:end_index])\n",
    "        groups.append(group)\n",
    "        start_index = end_index\n",
    "    return groups\n",
    "\n",
    "groups = split_dict_into_groups(item_to_inter, 8)\n",
    "\n",
    "for index, group in enumerate(groups):\n",
    "    print(f\"Group {index + 1}: {len(group)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "group_title = {}\n",
    "total_sum = 0\n",
    "group_prob = []\n",
    "for index, group in enumerate(groups):\n",
    "    group_title[index] = list(group.keys())\n",
    "    total_sum += sum(group.values())\n",
    "for index, group in enumerate(groups):\n",
    "    group_prob.append(sum(group.values())/total_sum)\n",
    "print(group_prob)\n",
    "with open(f'./info/{category}_group_title_8_{start_year}-10-{end_year}-11.json', mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(group_title, json_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_id = {}\n",
    "with open(f'./info/{category}_5_{start_year}-10-{end_year}-11.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split('\\t')\n",
    "        title_id['\\t'.join(parts[:-1])] = parts[-1]\n",
    "\n",
    "group_ids = {}\n",
    "for k,v in group_title.items():\n",
    "    group_ids[k] = []\n",
    "    for t in v:\n",
    "        group_ids[k].append(title_id[t])\n",
    "with open(f'./info/{category}_group_ids_8_{start_year}-10-{end_year}-11.json', mode='w', encoding='utf-8') as json_file:\n",
    "    json.dump(group_ids, json_file, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca_lora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59fa845e12d05d721e6f4368480cbf49d04f4a649a02e83c3e47bffdee3cc61a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
